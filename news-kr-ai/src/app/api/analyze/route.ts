import { NextRequest, NextResponse } from 'next/server';
import OpenAI from 'openai';
import { cache } from '../../../../lib/supabase-cache';

export const dynamic = 'force-dynamic'; // POST routes need dynamic handling

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY,
});

function getRelevanceIcons(score: number): string {
  if (score >= 9) return 'ðŸ”¥ðŸ”¥ðŸ”¥'; // 9-10: Critical relevance
  if (score >= 7) return 'ðŸ”¥ðŸ”¥';   // 7-8: High relevance  
  if (score >= 6) return 'ðŸ”¥';     // 6: Medium relevance
  return '';                       // <6: Not shown (filtered out)
}


export async function POST(request: NextRequest) {
  try {
    const { articles } = await request.json();
    
    const analyzedArticles = await Promise.all(
      articles.map(async (article: any) => {
        try {
          // Check cache first
          const cachedAnalysis = await cache.getAnalysis(article.title);
          if (cachedAnalysis) {
            console.log('Using cached analysis for:', article.title);
            return {
              ...article,
              relevanceScore: cachedAnalysis.score,
              relevanceIcons: getRelevanceIcons(cachedAnalysis.score),
              analysisReason: cachedAnalysis.reason,
              category: cachedAnalysis.category
            };
          }

          console.log('Calling OpenAI for:', article.title);
          const prompt = `
ë‹¤ìŒ ë‰´ìŠ¤ ì œëª©ì„ ë¶„ì„í•´ì„œ íŠ¹ê²€ ìˆ˜ì‚¬ì™€ì˜ ê´€ë ¨ì„±ì„ 0-10ì ìœ¼ë¡œ ì ìˆ˜ë¥¼ ë§¤ê²¨ì£¼ì„¸ìš”.

ì œëª©: "${article.title}"
í‚¤ì›Œë“œ: ${article.keyword}

ì ìˆ˜ ê¸°ì¤€:
- 0-3ì : íŠ¹ê²€ê³¼ ë¬´ê´€í•œ ì¼ë°˜ ì •ì¹˜ ë‰´ìŠ¤
- 4-6ì : íŠ¹ê²€ ê´€ë ¨ ì¸ë¬¼ì´ë‚˜ ë°°ê²½ ì–¸ê¸‰
- 7-8ì : íŠ¹ê²€ ìˆ˜ì‚¬ ì§„í–‰ ìƒí™© ê´€ë ¨
- 9-10ì : íŠ¹ê²€ í•µì‹¬ ìˆ˜ì‚¬ ë‚´ìš© (ê¸°ì†Œ, ì˜ìž¥, ì••ìˆ˜ìˆ˜ìƒ‰ ë“±)

ì‘ë‹µ í˜•ì‹ (JSON):
{
  "score": ìˆ«ìž,
  "reason": "ì ìˆ˜ ì´ìœ  í•œ ì¤„",
  "category": "í•µì‹¬ìˆ˜ì‚¬|ì§„í–‰ìƒí™©|ê´€ë ¨ì¸ë¬¼|ì¼ë°˜ì •ì¹˜"
}`;

          const completion = await openai.chat.completions.create({
            model: "gpt-5-nano",
            messages: [{ role: "user", content: prompt }],
          });

          const responseContent = completion.choices[0].message.content || '{"score": 5, "reason": "ë¶„ì„ ì‹¤íŒ¨", "category": "ì¼ë°˜ì •ì¹˜"}';
          console.log('OpenAI Response:', responseContent);
          
          const analysis = JSON.parse(responseContent);
          
          // Cache the analysis (permanent)
          await cache.setAnalysis(article.title, analysis);
          
          return {
            ...article,
            relevanceScore: analysis.score,
            relevanceIcons: getRelevanceIcons(analysis.score),
            analysisReason: analysis.reason,
            category: analysis.category
          };
        } catch (error) {
          console.error('Error analyzing article:', error);
          return {
            ...article,
            relevanceScore: 5,
            relevanceIcons: '',
            analysisReason: "ë¶„ì„ ì‹¤íŒ¨",
            category: "ì¼ë°˜ì •ì¹˜"
          };
        }
      })
    );

    const filteredArticles = analyzedArticles
      .filter(article => article.relevanceScore >= 6)
      .sort((a, b) => b.relevanceScore - a.relevanceScore);

    return NextResponse.json({
      analyzed: analyzedArticles.length,
      filtered: filteredArticles.length,
      articles: filteredArticles
    });

  } catch (error) {
    console.error('Error in analysis:', error);
    return NextResponse.json(
      { error: 'Failed to analyze articles' },
      { status: 500 }
    );
  }
}